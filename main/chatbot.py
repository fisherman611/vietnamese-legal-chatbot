from typing import List, Dict, Any, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage, SystemMessage

from main.vector_store import QdrantVectorStore
from main.bm25_retriever import BM25Retriever
from main.reranker import DocumentReranker
from utils.text_processor import VietnameseTextProcessor
from utils.google_search import GoogleSearchTool
from utils.question_refiner import VietnameseLegalQuestionRefiner
from config import Config

class VietnameseLegalRAG:
    """Vietnamese Legal RAG System"""
    
    def __init__(self):
        self.vector_store = None
        self.bm25_retriever = None
        self.reranker = None
        self.llm = None
        self.text_processor = VietnameseTextProcessor()
        self.google_search = GoogleSearchTool()
        self.question_refiner = VietnameseLegalQuestionRefiner()
        
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize RAG components"""
        try:
            # Initialize LLM
            if Config.GOOGLE_API_KEY:
                self.llm = ChatGoogleGenerativeAI(
                    model=Config.MODEL_GEN,
                    google_api_key=Config.GOOGLE_API_KEY,
                    temperature=0.1
                )
                print("Google Gemini LLM initialized")
            else:
                print("Warning: Google API key not found")
            
            # Initialize vector store
            self.vector_store = QdrantVectorStore()
            
            # Initialize BM25 retriever
            self.bm25_retriever = BM25Retriever()
            
            # Initialize reranker if enabled
            if Config.ENABLE_RERANKING:
                self.reranker = DocumentReranker(model_name=Config.RERANKER_MODEL)
            else:
                print("Reranking disabled in configuration")
            
        except Exception as e:
            print(f"Error initializing RAG components: {e}")
    
    def setup_indices(self, documents: List[Dict[str, Any]], force_rebuild: bool = False):
        """Setup both vector and BM25 indices"""
        print("Setting up RAG indices...")
        
        try:
            # Setup vector store
            if self.vector_store:
                # Check if we need to create collection
                try:
                    # First, do a simple existence check
                    collections = self.vector_store.client.get_collections().collections
                    collection_exists = any(col.name == self.vector_store.collection_name for col in collections)
                    print(f"Collection existence check: {collection_exists}")
                    
                    if collection_exists:
                        # Collection exists, try to get detailed info
                        try:
                            collection_info = self.vector_store.get_collection_info()
                            has_documents = collection_info.get('points_count', 0) > 0
                            
                            if force_rebuild:
                                print("Force rebuild requested - recreating vector store...")
                                self.vector_store.create_collection(force_recreate=True)
                                self.vector_store.add_documents(documents)
                            elif not has_documents:
                                print("Collection exists but is empty - adding documents...")
                                self.vector_store.add_documents(documents)
                            else:
                                print(f"Vector store collection already exists with {collection_info.get('points_count', 0)} documents")
                        except Exception as info_e:
                            print(f"Could not get collection info: {info_e}")
                            if force_rebuild:
                                print("Force rebuild requested - recreating vector store...")
                                self.vector_store.create_collection(force_recreate=True)
                                self.vector_store.add_documents(documents)
                            else:
                                print("Assuming collection has documents - skipping setup")
                    else:
                        # Collection doesn't exist, create it
                        print("Collection does not exist - creating new collection...")
                        self.vector_store.create_collection(force_recreate=False)
                        self.vector_store.add_documents(documents)
                        
                except Exception as e:
                    print(f"Error during vector store setup: {e}")
                    print("Attempting to create collection...")
                    self.vector_store.create_collection()
                    self.vector_store.add_documents(documents)
            
            # Setup BM25 index
            if self.bm25_retriever:
                # Try to load existing index
                if not self.bm25_retriever.load_index() or force_rebuild:
                    self.bm25_retriever.build_index(documents)
                    self.bm25_retriever.save_index()
                else:
                    print("BM25 index loaded from file")
            
            print("RAG indices setup completed")
            
        except Exception as e:
            print(f"Error setting up indices: {e}")
            raise
    
    def retrieve_documents(self, query: str, use_hybrid: bool = True, use_reranking: bool = None) -> List[Dict[str, Any]]:
        """Retrieve relevant documents using hybrid approach with optional reranking"""
        retrieved_docs = []
        
        # Use config default if not specified
        if use_reranking is None:
            use_reranking = Config.ENABLE_RERANKING
        
        # Adjust retrieval counts if reranking is enabled
        bm25_top_k = Config.RERANK_BEFORE_RETRIEVAL_TOP_K if use_reranking else Config.BM25_TOP_K
        vector_top_k = Config.RERANK_BEFORE_RETRIEVAL_TOP_K if use_reranking else Config.TOP_K_RETRIEVAL
        
        try:
            if use_hybrid and self.bm25_retriever and self.vector_store:
                # Hybrid retrieval: BM25 + Vector Search
                
                # BM25 retrieval
                bm25_results = self.bm25_retriever.get_relevant_documents(
                    query, top_k=bm25_top_k
                )
                
                # Vector search
                vector_results = self.vector_store.search_similar_documents(
                    query, top_k=vector_top_k
                )
                
                # Combine and deduplicate results
                all_docs = {}
                
                # Add BM25 results
                for doc in bm25_results:
                    doc_id = doc.get('id', '')
                    if doc_id:
                        all_docs[doc_id] = {**doc, 'retrieval_method': 'bm25'}
                
                # Add vector results
                for doc in vector_results:
                    doc_id = doc.get('id', '')
                    if doc_id:
                        if doc_id in all_docs:
                            # Combine scores if document found by both methods
                            all_docs[doc_id]['retrieval_method'] = 'hybrid'
                            all_docs[doc_id]['vector_score'] = doc.get('score', 0)
                        else:
                            all_docs[doc_id] = {**doc, 'retrieval_method': 'vector'}
                
                retrieved_docs = list(all_docs.values())
                
            elif self.vector_store:
                # Vector search only
                retrieved_docs = self.vector_store.search_similar_documents(query, top_k=vector_top_k)
                
            elif self.bm25_retriever:
                # BM25 only
                retrieved_docs = self.bm25_retriever.get_relevant_documents(query, top_k=bm25_top_k)
            
            # Improved similarity filtering logic
            if retrieved_docs:
                # Apply reranking FIRST if enabled (before similarity filtering)
                if use_reranking and self.reranker and retrieved_docs:
                    print(f"Applying reranking to {len(retrieved_docs)} documents...")
                    
                    if Config.USE_SCORE_FUSION:
                        # Use score fusion for better results
                        retrieved_docs = self.reranker.rerank_with_fusion(
                            query, 
                            retrieved_docs, 
                            alpha=Config.RERANKER_FUSION_ALPHA,
                            top_k=Config.RERANKER_TOP_K
                        )
                    else:
                        # Use pure reranker scores
                        retrieved_docs = self.reranker.rerank_documents(
                            query, 
                            retrieved_docs, 
                            top_k=Config.RERANKER_TOP_K
                        )
                    print(f"Reranking completed, returning {len(retrieved_docs)} documents")
                    print([(retrieved_doc['id'], retrieved_doc['score']) for retrieved_doc in retrieved_docs])
                    return retrieved_docs
                
                # Check for high-quality documents first (if no reranking)
                high_quality_docs = []
                moderate_quality_docs = []
                
                for doc in retrieved_docs:
                    score = doc.get('score', 0)
                    if score >= Config.SIMILARITY_THRESHOLD:
                        high_quality_docs.append(doc)
                    elif score >= Config.MIN_SIMILARITY_FOR_LEGAL_DOCS:
                        moderate_quality_docs.append(doc)
                
                # Return high quality docs if available
                if high_quality_docs:
                    print(f"Retrieved {len(high_quality_docs)} high-quality documents")
                    print([(high_quality_doc['id'], high_quality_doc['score']) for high_quality_doc in high_quality_docs])
                    return high_quality_docs[:Config.TOP_K_RETRIEVAL]
                
                # Return moderate quality docs if no high quality ones
                elif moderate_quality_docs:
                    print(f"Retrieved {len(moderate_quality_docs)} moderate-quality documents")
                    print([(moderate_quality_doc['id'], moderate_quality_doc['score']) for moderate_quality_doc in moderate_quality_docs])
                    return moderate_quality_docs[:Config.TOP_K_RETRIEVAL]
                
                else:
                    print("No documents found with sufficient similarity scores")
                    return []
            else:
                # No documents retrieved
                return []
            
        except Exception as e:
            print(f"Error retrieving documents: {e}")
            return []
    
    def format_context(self, documents: List[Dict[str, Any]]) -> str:
        """Format retrieved documents as context for LLM"""
        if not documents:
            return "KhÃ´ng cÃ³ tÃ i liá»‡u phÃ¡p luáº­t liÃªn quan Ä‘Æ°á»£c tÃ¬m tháº¥y."
        
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            title = doc.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»')
            content = doc.get('content', '')
            doc_id = doc.get('id', '')
            metadata = doc.get('metadata', {})
            law_id = metadata.get('law_id', '')
            article_id = metadata.get('article_id', '')
            
            # Limit content length
            if len(content) > 500:
                content = content[:500] + "..."
            
            # Format law and article information
            law_info = f"Luáº­t: {law_id}" if law_id else ""
            article_info = f"Äiá»u {article_id}" if article_id else f"ID: {doc_id}"
            
            context_part = f"""
TÃ i liá»‡u {i}:
{law_info}
{article_info}: {title}
Ná»™i dung: {content}
"""
            context_parts.append(context_part.strip())
        
        return "\n\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str, is_fallback: bool = False) -> str:
        """Generate answer using LLM with context"""
        if not self.llm:
            return "Lá»—i: KhÃ´ng thá»ƒ káº¿t ná»‘i vá»›i mÃ´ hÃ¬nh ngÃ´n ngá»¯."
        
        try:
            # Create prompt based on scenario
            if is_fallback:
                prompt = Config.FALLBACK_SYSTEM_PROMPT.format(
                    context=context,
                    question=query
                )
            else:
                prompt = Config.SYSTEM_PROMPT.format(
                    context=context,
                    question=query
                )
            
            # Generate response
            messages = [HumanMessage(content=prompt)]
            response = self.llm.invoke(messages)
            
            answer = response.content
            
            # Add legal disclaimer if enabled and it's a fallback response
            if Config.SHOW_LEGAL_DISCLAIMER and is_fallback:
                disclaimer = "\n\nLÆ°u Ã½ quan trá»ng: Äá»ƒ Ä‘áº£m báº£o quyá»n lá»£i cá»§a mÃ¬nh, ngÆ°á»i lao Ä‘á»™ng nÃªn tÃ¬m Ä‘áº¿n cÃ¡c chuyÃªn gia phÃ¡p lÃ½ hoáº·c cÆ¡ quan chá»©c nÄƒng cÃ³ tháº©m quyá»n Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n cá»¥ thá»ƒ vÃ  chÃ­nh xÃ¡c nháº¥t dá»±a trÃªn tÃ¬nh hÃ¬nh thá»±c táº¿ cá»§a mÃ¬nh."
                answer += disclaimer
            
            return answer
            
        except Exception as e:
            print(f"Error generating answer: {e}")
            return f"Lá»—i khi táº¡o cÃ¢u tráº£ lá»i: {str(e)}"
    
    def _is_negative_response(self, response: str) -> bool:
        """Check if the response is a negative/unable to answer response"""
        negative_indicators = [
            "khÃ´ng thá»ƒ tráº£ lá»i",
            "khÃ´ng tÃ¬m tháº¥y",
            "khÃ´ng cÃ³ thÃ´ng tin",
            "xin lá»—i",
            "khÃ´ng thá»ƒ tÃ¬m tháº¥y",
            "khÃ´ng cÃ³ dá»¯ liá»‡u",
            "khÃ´ng rÃµ",
            "khÃ´ng biáº¿t",
            "khÃ´ng Ä‘á»§ thÃ´ng tin",
            "thiáº¿u thÃ´ng tin",
            "khÃ´ng cÃ³ trong",
            "ngoÃ i pháº¡m vi",
            # Add the specific pattern mentioned by user
            "khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin trong tÃ i liá»‡u tham kháº£o Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i nÃ y",
            "cáº§n tham kháº£o thÃªm cÃ¡c vÄƒn báº£n phÃ¡p luáº­t khÃ¡c",
            "tÃ¬m kiáº¿m thÃ´ng tin chuyÃªn sÃ¢u hÆ¡n vá»",
            "tÃ i liá»‡u tham kháº£o khÃ´ng chá»©a thÃ´ng tin Ä‘áº§y Ä‘á»§"
        ]
        
        response_lower = response.lower()
        return any(indicator in response_lower for indicator in negative_indicators)
    
    def answer_question(self, query: str, use_fallback: bool = True, refine_question: bool = True) -> Dict[str, Any]:
        """Answer a legal question using RAG with enhanced negative response handling and question refinement"""
        print(f"Processing question: {query}")

        # Step 1: Refine the question if enabled
        original_query = query
        refinement_result = None
        
        if refine_question and Config.ENABLE_QUESTION_REFINEMENT and self.question_refiner:
            print("ðŸ”§ Refining question for better search accuracy...")
            refinement_result = self.question_refiner.refine_question(query, use_llm=Config.USE_LLM_FOR_REFINEMENT)
            
            if refinement_result["refined_question"] != query:
                refined_query = refinement_result["refined_question"]
                print(f"ðŸ“ Original: {query}")
                print(f"âœ¨ Refined: {refined_query}")
                query = refined_query

        # Step 2: Retrieve relevant documents using refined query
        retrieved_docs = self.retrieve_documents(query)
        
        # Check if we have relevant documents
        if not retrieved_docs and Config.ENABLE_GOOGLE_SEARCH and use_fallback:
            print("No relevant legal documents found, using Google search fallback")
            
            # Use Google search as fallback
            search_results = self.google_search.search_legal_info(query)
            
            if search_results:
                fallback_context = self.google_search.format_search_results(search_results)
                
                # Generate answer with fallback context
                fallback_answer = self.generate_answer(query, fallback_context, True)
                
                return {
                    'answer': fallback_answer,
                    'retrieved_documents': [],
                    'fallback_used': True,
                    'search_results': search_results,
                    'context': fallback_context,
                    'search_results_html': self.google_search.format_search_results_for_display(search_results),
                    'original_question': original_query,
                    'refined_question': query,
                    'question_refinement': refinement_result
                }
            else:
                return {
                    'answer': "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¡p luáº­t liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n trong cÆ¡ sá»Ÿ dá»¯ liá»‡u ná»™i bá»™ vÃ  cÅ©ng khÃ´ng thá»ƒ tÃ¬m kiáº¿m thÃ´ng tin trÃªn web. Vui lÃ²ng thá»­ láº¡i vá»›i cÃ¢u há»i khÃ¡c hoáº·c liÃªn há»‡ vá»›i chuyÃªn gia phÃ¡p lÃ½.",
                    'retrieved_documents': [],
                    'fallback_used': True,
                    'search_results': [],
                    'context': "",
                    'search_results_html': "",
                    'original_question': original_query,
                    'refined_question': query,
                    'question_refinement': refinement_result
                }
        elif not retrieved_docs:
            return {
                'answer': "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¡p luáº­t liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.",
                'retrieved_documents': [],
                'fallback_used': False,
                'context': "",
                'search_results': [],
                'search_results_html': "",
                'original_question': original_query,
                'refined_question': query,
                'question_refinement': refinement_result
            }

        # Format context
        context = self.format_context(retrieved_docs)

        # Generate answer
        answer = self.generate_answer(query, context, False)

        # Check if the generated answer is negative and retry with Google search
        if self._is_negative_response(answer) and use_fallback:
            print("ðŸ” Detected insufficient information response, activating search tools...")
            
            # Inform user that search is being performed
            search_notification = f"\n\n*ðŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin bá»• sung Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n...*"
            
            # Try Google search if enabled
            if Config.ENABLE_GOOGLE_SEARCH:
                print("ðŸ“¡ Trying web search...")
                search_results = self.google_search.search_legal_info(query)
                
                if search_results:
                    # Generate enhanced response with web information
                    web_context = self.google_search.format_search_results(search_results)
                    combined_context = context + "\n\nThÃ´ng tin bá»• sung tá»« web:\n" + web_context
                    enhanced_answer = self.generate_answer(query, combined_context, True)
                    
                    return {
                        'answer': enhanced_answer,
                        'retrieved_documents': retrieved_docs,
                        'fallback_used': True,
                        'context': combined_context,
                        'search_results': search_results,
                        'search_results_html': self.google_search.format_search_results_for_display(search_results),
                        'search_triggered': True,
                        'original_question': original_query,
                        'refined_question': query,
                        'question_refinement': refinement_result
                    }
                else:
                    # Google search found nothing useful
                    return {
                        'answer': answer + "\n\n*âš ï¸ TÃ´i Ä‘Ã£ cá»‘ gáº¯ng tÃ¬m kiáº¿m thÃªm thÃ´ng tin trÃªn web nhÆ°ng khÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p. Äá»ƒ cÃ³ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n, báº¡n cÃ³ thá»ƒ tham kháº£o Ã½ kiáº¿n chuyÃªn gia phÃ¡p lÃ½.*",
                        'retrieved_documents': retrieved_docs,
                        'fallback_used': True,
                        'context': context,
                        'search_results': [],
                        'search_results_html': "",
                        'search_triggered': True,
                        'original_question': original_query,
                        'refined_question': query,
                        'question_refinement': refinement_result
                    }
            else:
                # Google search disabled
                return {
                    'answer': answer + "\n\n*âš ï¸ Äá»ƒ cÃ³ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n, báº¡n cÃ³ thá»ƒ tham kháº£o Ã½ kiáº¿n chuyÃªn gia phÃ¡p lÃ½.*",
                    'retrieved_documents': retrieved_docs,
                    'fallback_used': False,
                    'context': context,
                    'search_results': [],
                    'search_results_html': "",
                    'original_question': original_query,
                    'refined_question': query,
                    'question_refinement': refinement_result
                }

        # Return successful result
        return {
            'answer': answer,
            'retrieved_documents': retrieved_docs,
            'fallback_used': False,
            'context': context,
            'search_results': [],
            'search_results_html': "",
            'original_question': original_query,
            'refined_question': query,
            'question_refinement': refinement_result
        }
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get status of RAG system components"""
        status = {
            'llm_available': self.llm is not None,
            'vector_store_available': self.vector_store is not None,
            'bm25_available': self.bm25_retriever is not None,
            'reranker_available': self.reranker is not None and self.reranker.model is not None,
            'reranking_enabled': Config.ENABLE_RERANKING,
            'google_api_configured': bool(Config.GOOGLE_API_KEY),
            'qdrant_configured': bool(Config.QDRANT_URL and Config.QDRANT_API_KEY)
        }
        
        # Get collection info if available
        if self.vector_store:
            try:
                status['vector_store_info'] = self.vector_store.get_collection_info()
            except:
                status['vector_store_info'] = {}
        
        # Get BM25 stats if available
        if self.bm25_retriever:
            status['bm25_stats'] = self.bm25_retriever.get_index_stats()
        
        # Get reranker info if available
        if self.reranker:
            status['reranker_info'] = self.reranker.get_model_info()
        
        return status 